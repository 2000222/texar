{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a tutorial on how to load your data and experiments on the BERT pretrain models. Take the Stanford Sentiment Tree Bank as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, we import the necessary modules first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import texar as tx\n",
    "from texar.modules import TransformerEncoder\n",
    "from texar.utils.mode import is_train_mode\n",
    "from texar.core import get_train_op\n",
    "from utils import data_utils, model_utils, tokenization\n",
    "from data_utils import SSTProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "After running `python download_glue_data.py`, you should already have the SST data in `data/SST-2` directory.\n",
    "The data has been processed in a standard format. \n",
    "You can view the original files in `data/SST-2/original` directory. The data has been splitted into train/dev/test and transformed to tab-seperated tsv format. \n",
    "\n",
    "Here we show some samples of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------train sample--------------\n",
      "sentence\tlabel\n",
      "\n",
      "hide new secretions from the parental units \t0\n",
      "\n",
      "contains no wit , only labored gags \t0\n",
      "\n",
      "-------------evaluate sample--------------\n",
      "sentence\tlabel\n",
      "\n",
      "it 's a charming and often affecting journey . \t1\n",
      "\n",
      "unflinchingly bleak and desperate \t0\n",
      "\n",
      "-------------test sample--------------\n",
      "index\tsentence\n",
      "\n",
      "0\tuneasy mishmash of styles and genres .\n",
      "\n",
      "1\tthis film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-------------train sample--------------')\n",
    "with open('data/SST-2/train.tsv') as fin:\n",
    "    for i in range(3):\n",
    "        print(next(fin))\n",
    "\n",
    "print('-------------evaluate sample--------------')\n",
    "with open('data/SST-2/dev.tsv') as fin:\n",
    "    for i in range(3):\n",
    "        print(next(fin))\n",
    "        \n",
    "print('-------------test sample--------------')\n",
    "with open('data/SST-2/test.tsv') as fin:\n",
    "    for i in range(3):\n",
    "        print(next(fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data and evaluation data are in the same schema: the first line gives the header information, `sentence` and `label`. In the following lines, the sentence is a space-seperated string, and the label is `0` or `1`.\n",
    "The test data has different schemas, where the first column is a unique index for each test example, the second column is the space-seperated string.\n",
    "\n",
    "In the `utils/data_utils`, there are five types of Data Processor Implemented. We have tried the `MrpcProcessor` in the `bert_classifier_main.py` pipeline for sentence pair classification. For this SST single sentence classification input, we use the `SSTProcessor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we specify the data configuration and obtain the train/dev/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the pretrained BERT model architecture to be used\")\n",
    "bert_pretrain_config = 'uncased_L-12_H-768_A-12'\n",
    "# specify Whether to lower case the input text. Should be True for uncased \n",
    "# models and False for cased models.\n",
    "do_lower_case = True\n",
    "# Specify the output directory\n",
    "output_dir = 'sst_output'\n",
    "\n",
    "tf.gfile.MakeDirs(output_dir)\n",
    "\n",
    "processor = SSTProcessor()\n",
    "\n",
    "# The following is the same as Mrpc examples in `bert_classifier_main.py`\n",
    "# except that the tf.FLAGS and config_data has been replaced b variables.\n",
    "num_labels = len(processor.get_labels())\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='bert_pretrained_models/%s/vocab.txt'\n",
    "        %(bert_pretrain_config),\n",
    "    do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "# The following hyperparameters can be writtern in config_data.py\n",
    "data_dir = 'data/SST-2/'\n",
    "max_seq_length = 128\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 8\n",
    "test_batch_size = 8\n",
    "max_train_epoch = 3\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "train_dataset = data_utils.get_dataset(processor, tokenizer, data_dir,\n",
    "    max_seq_length, train_batch_size,\n",
    "    mode='train', output_dir=output_dir)\n",
    "eval_dataset = data_utils.get_dataset(processor, tokenizer, data_dir,\n",
    "    max_seq_length, eval_batch_size,\n",
    "    mode='eval', output_dir=output_dir)\n",
    "test_dataset = data_utils.get_dataset(processor, tokenizer, data_dir,\n",
    "    max_seq_length, test_batch_size,\n",
    "    mode='test', output_dir=output_dir)\n",
    "\n",
    "iterator = tx.data.FeedableDataIterator({\n",
    "    'train': train_dataset,\n",
    "    'eval': eval_dataset,\n",
    "    'test': test_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part to build the model should be generally the same as in `bert_classifier_main.py`. # We may want to change the hyperparameters in `config_model` to improve the performance. Recall that the hparams in `config_model` control the downstream model architecture and the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = importlib.import_module(\n",
    "    'bert_config_lib.config_model_%s' % (bert_pretrain_config))\n",
    "\n",
    "config_model = importlib.import_module('config_classifier')\n",
    "\n",
    "batch = iterator.get_next()\n",
    "input_ids = batch[\"input_ids\"]\n",
    "segment_ids = batch[\"segment_ids\"]\n",
    "\n",
    "batch_size = tf.shape(input_ids)[0]\n",
    "input_length = tf.reduce_sum(\n",
    "    1 - tf.to_int32(tf.equal(input_ids, 0)), axis=1)\n",
    "\n",
    "# BERT (Transformer) model configuration\n",
    "mode = None # to follow the global mode\n",
    "with tf.variable_scope('bert'):\n",
    "    \n",
    "    # create the word embedding for the word tokens \n",
    "    embedder = tx.modules.WordEmbedder(\n",
    "        vocab_size=bert_config.vocab_size,\n",
    "        hparams=bert_config.embed)\n",
    "    word_embeds = embedder(input_ids, mode=mode)\n",
    "    \n",
    "    # create the token type embeddings for each type of of tokens.\n",
    "    # For sentence pair classification, each sentence pair will be assigned different\n",
    "    # token type embedding based on their segment ids.\n",
    "    token_type_embedder = tx.modules.WordEmbedder(\n",
    "        vocab_size=bert_config.type_vocab_size,\n",
    "        hparams=bert_config.token_type_embed)\n",
    "    token_type_embeds = token_type_embedder(segment_ids, mode=mode)\n",
    "    \n",
    "    # add word embedding and token type embedding to obtain the hidden\n",
    "    # representation of each word\n",
    "    input_embeds = word_embeds + token_type_embeds\n",
    "    \n",
    "    # Transformer encoder to obtain the hidden representation of each word in sentence\n",
    "    encoder = TransformerEncoder(hparams=bert_config.encoder)\n",
    "    output = encoder(input_embeds, input_length, mode=mode)\n",
    "\n",
    "    with tf.variable_scope(\"pooler\"):\n",
    "        # Use the projection of first token hidden vector of BERT output\n",
    "        # as the representation of the sentence\n",
    "        bert_sent_hidden = tf.squeeze(output[:, 0:1, :], axis=1)\n",
    "        bert_sent_output = tf.layers.dense(\n",
    "            bert_sent_hidden, config_model.hidden_dim, activation=tf.tanh)\n",
    "        output = tf.layers.dropout(bert_sent_output, rate=0.1,\n",
    "        training=is_train_mode(mode))\n",
    "\n",
    "# Downstream model configuration, obtain the logits and probabilities for classification\n",
    "logits = tf.layers.dense(output, num_labels,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we define the model training operation and some statistics for evaluating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "# Losses & train_ops\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "    labels=batch[\"label_ids\"], logits=logits)\n",
    "\n",
    "# Calculate the dynamic learning rate based on global step\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "static_lr = config_model.lr['static_lr']\n",
    "num_train_steps = int(len(train_examples) / train_batch_size \\\n",
    "    * max_train_epoch)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "lr = model_utils.get_lr(global_step, num_train_steps, num_warmup_steps, static_lr)\n",
    "\n",
    "# Get training operation\n",
    "train_op = get_train_op(\n",
    "    loss,\n",
    "    global_step=global_step,\n",
    "    learning_rate=lr,\n",
    "    hparams=config_model.opt)\n",
    "\n",
    "# Monitering data\n",
    "accu = tx.evals.accuracy(batch['label_ids'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we can specify what we should do in one train/dev/test epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(sess, mode):\n",
    "    fetches = {\n",
    "        'accu': accu,\n",
    "        'batch_size': batch_size,\n",
    "        'step': global_step,\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    if mode == 'train':\n",
    "        ### in training epoch, we need to run the train_op to update parameters\n",
    "        fetches['train_op'] = train_op\n",
    "        while True:\n",
    "            try:\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, 'train'),\n",
    "                    tx.context.global_mode(): tf.estimator.ModeKeys.TRAIN,\n",
    "                }\n",
    "                rets = sess.run(fetches, feed_dict)\n",
    "                #if rets['step'] % 50 == 0:\n",
    "                tf.logging.info('step:%d loss:%f' % (\n",
    "                    rets['step'], rets['loss']))\n",
    "                if rets['step'] == num_train_steps:\n",
    "                    break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "    if mode == 'eval':\n",
    "        cum_acc = 0.0 # cumulative number of correct prediction\n",
    "        nsamples = 0\n",
    "        while True:\n",
    "            try:\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, 'eval'),\n",
    "                    tx.context.global_mode(): tf.estimator.ModeKeys.EVAL,\n",
    "                }\n",
    "                rets = sess.run(fetches, feed_dict)\n",
    "                cum_acc += rets['accu'] * rets['batch_size']\n",
    "                nsamples += rets['batch_size']\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        tf.logging.info('evaluation accuracy:{}'.format(cum_acc / nsamples))\n",
    "\n",
    "    if mode == 'test':\n",
    "        _all_probs = []\n",
    "        while True:\n",
    "            try:\n",
    "                feed_dict = {\n",
    "                    iterator.handle: iterator.get_handle(sess, 'test'),\n",
    "                    tx.context.global_mode(): tf.estimator.ModeKeys.PREDICT,\n",
    "                }\n",
    "                _probs = sess.run(probs, feed_dict=feed_dict)\n",
    "                _all_probs.extend(_probs.tolist())\n",
    "            except:\n",
    "                brea\n",
    "        # Predicted probabilities are saved in a tsv file.      \n",
    "        # Each line will contain output for each sample, \n",
    "        # with two fields representing the probabilities for each class.\n",
    "        output_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
    "        with tf.gfile.GFile(output_file, \"w\") as writer:\n",
    "            for prediction in _all_probs:\n",
    "                output_line = \"\\t\".join(\n",
    "                    str(class_probability) for class_probability in prediction) + \"\\n\"\n",
    "                writer.write(output_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the following, we give the introductions of how to run the training, evaluation and test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = False # If you want to run train epoch, set it to True\n",
    "do_eval = False # If you want to run eval epoch, set it to True\n",
    "do_test = False # If you want to run test epoch, set it to True\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load Pretrained BERT model parameters\n",
    "    init_checkpoint='bert_pretrained_models/%s/bert_model.ckpt' % bert_pretrain_config\n",
    "    \n",
    "    # Load the pretrained BERT model\n",
    "    if init_checkpoint:\n",
    "        model_utils.init_bert_checkpoint(init_checkpoint)\n",
    "        \n",
    "    # Initialize all the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    # Restore trained model if specified\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "    iterator.initialize_dataset(sess)\n",
    "    if do_train:\n",
    "        iterator.restart_dataset(sess, 'train')\n",
    "        _run_epoch(sess, mode='train')\n",
    "        saver.save(sess, FLAGS.output_dir + '/model.ckpt')\n",
    "\n",
    "    if do_eval:\n",
    "        iterator.restart_dataset(sess, 'eval')\n",
    "        _run_epoch(sess, mode='eval')\n",
    "\n",
    "    if do_test:\n",
    "        iterator.restart_dataset(sess, 'test')\n",
    "        _run_epoch(sess, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
